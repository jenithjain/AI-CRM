{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e2eb74a-3d57-4b97-b65b-4d7ace097f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "try:\n",
    "    client = MongoClient('***')\n",
    "    db =client['CRM']\n",
    "    print(\"Connected to MongoDB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e5d6ab-04fe-4fc1-9a13-de23dc9f8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Generate Leads data\n",
    "leads = []\n",
    "for _ in range(100):\n",
    "    leads.append({\n",
    "        \"lead_id\": fake.uuid4(),\n",
    "        \"name\": fake.name(),\n",
    "        \"email\": fake.email(),\n",
    "        \"phone\": fake.phone_number(),\n",
    "        \"created_at\": datetime.now(),\n",
    "        \"status\": random.choice([\"New\", \"Contacted\", \"Qualified\", \"Lost\"])\n",
    "    })\n",
    "db[\"Leads\"].insert_many(leads)\n",
    "\n",
    "# Generate Sales data\n",
    "sales = []\n",
    "for _ in range(100):\n",
    "    sales.append({\n",
    "        \"sale_id\": fake.uuid4(),\n",
    "        \"lead_id\": random.choice(leads)[\"lead_id\"],\n",
    "        \"product_id\": fake.uuid4(),\n",
    "        \"amount\": round(random.uniform(100, 1000), 2),\n",
    "        \"sale_date\": datetime.now() - timedelta(days=random.randint(0, 30)),\n",
    "        \"quantity\": random.randint(1, 10)\n",
    "    })\n",
    "db[\"Sales\"].insert_many(sales)\n",
    "\n",
    "# Generate Campaign data\n",
    "campaigns = []\n",
    "for _ in range(10):\n",
    "    campaigns.append({\n",
    "        \"campaign_id\": fake.uuid4(),\n",
    "        \"name\": fake.catch_phrase(),\n",
    "        \"start_date\": datetime.now() - timedelta(days=random.randint(0, 90)),\n",
    "        \"end_date\": datetime.now() - timedelta(days=random.randint(0, 30)),\n",
    "        \"budget\": round(random.uniform(1000, 10000), 2),\n",
    "        \"status\": random.choice([\"Active\", \"Completed\", \"Cancelled\"])\n",
    "    })\n",
    "db[\"campaing\"].insert_many(campaigns)\n",
    "\n",
    "# Generate Customer Interaction data\n",
    "interactions = []\n",
    "for _ in range(200):\n",
    "    interactions.append({\n",
    "        \"interaction_id\": fake.uuid4(),\n",
    "        \"lead_id\": random.choice(leads)[\"lead_id\"],\n",
    "        \"interaction_type\": random.choice([\"Call\", \"Email\", \"Meeting\"]),\n",
    "        \"interaction_date\": datetime.now() - timedelta(days=random.randint(0, 30)),\n",
    "        \"notes\": fake.sentence()\n",
    "    })\n",
    "db[\"customer_interaction\"].insert_many(interactions)\n",
    "\n",
    "# Generate Product data\n",
    "products = []\n",
    "for _ in range(50):\n",
    "    products.append({\n",
    "        \"product_id\": fake.uuid4(),\n",
    "        \"name\": fake.word(),\n",
    "        \"category\": random.choice([\"Beverage\", \"Snack\", \"Dairy\", \"Frozen\", \"Grocery\"]),\n",
    "        \"price\": round(random.uniform(1, 100), 2),\n",
    "        \"stock\": random.randint(0, 1000)\n",
    "    })\n",
    "db[\"product\"].insert_many(products)\n",
    "\n",
    "# Generate Retention data\n",
    "retention = []\n",
    "for _ in range(100):\n",
    "    retention.append({\n",
    "        \"retention_id\": fake.uuid4(),\n",
    "        \"customer_id\": fake.uuid4(),\n",
    "        \"purchase_date\": datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "        \"last_interaction_date\": datetime.now() - timedelta(days=random.randint(0, 30)),\n",
    "        \"status\": random.choice([\"Active\", \"Churned\"])\n",
    "    })\n",
    "db[\"retention\"].insert_many(retention)\n",
    "\n",
    "# Generate Task data\n",
    "tasks = []\n",
    "for _ in range(50):\n",
    "    tasks.append({\n",
    "        \"task_id\": fake.uuid4(),\n",
    "        \"description\": fake.sentence(),\n",
    "        \"assigned_to\": fake.name(),\n",
    "        \"due_date\": datetime.now() + timedelta(days=random.randint(1, 30)),\n",
    "        \"status\": random.choice([\"Pending\", \"In Progress\", \"Completed\"])\n",
    "    })\n",
    "db[\"task\"].insert_many(tasks)\n",
    "\n",
    "# Generate User Access data\n",
    "user_access = []\n",
    "for _ in range(10):\n",
    "    user_access.append({\n",
    "        \"user_id\": fake.uuid4(),\n",
    "        \"username\": fake.user_name(),\n",
    "        \"password\": fake.password(),\n",
    "        \"access_level\": random.choice([\"Admin\", \"User\", \"Viewer\"]),\n",
    "        \"last_login\": datetime.now() - timedelta(days=random.randint(0, 30))\n",
    "    })\n",
    "db[\"user_access\"].insert_many(user_access)\n",
    "\n",
    "print(\"Data inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e963965-e922-460e-9aa1-81b342d3ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 sales records inserted into the sales collection.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Connect to MongoDB\n",
    "# client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# db = client[\"fmcg_database\"]\n",
    "sales_collection = db[\"sales\"]\n",
    "\n",
    "# Function to generate random sales data with revenue\n",
    "def generate_sales_data(num_entries):\n",
    "    sales_data = []\n",
    "    for i in range(1, num_entries + 1):\n",
    "        # Generate random product and customer IDs\n",
    "        sale_id = f\"S{i:03}\"\n",
    "        product_id = f\"P{random.randint(1, 50):03}\"\n",
    "        customer_id = f\"C{random.randint(1, 100):03}\"\n",
    "\n",
    "        # Randomize quantity and price, then calculate revenue\n",
    "        quantity = random.randint(1, 100)\n",
    "        price_per_unit = round(random.uniform(10.0, 50.0), 2)\n",
    "        revenue = round(quantity * price_per_unit, 2)\n",
    "\n",
    "        # Create sale date in a random past date within this year\n",
    "        sale_date = datetime(2024, random.randint(1, 10), random.randint(1, 28))\n",
    "\n",
    "        # Compile sale document\n",
    "        sale_document = {\n",
    "            \"sale_id\": sale_id,\n",
    "            \"product_id\": product_id,\n",
    "            \"customer_id\": customer_id,\n",
    "            \"quantity\": quantity,\n",
    "            \"price_per_unit\": price_per_unit,\n",
    "            \"revenue\": revenue,\n",
    "            \"sale_date\": sale_date\n",
    "        }\n",
    "\n",
    "        sales_data.append(sale_document)\n",
    "    \n",
    "    return sales_data\n",
    "\n",
    "# Generate 100 sample sales records and insert into MongoDB\n",
    "sales_data = generate_sales_data(100)\n",
    "sales_collection.insert_many(sales_data)\n",
    "\n",
    "print(f\"{len(sales_data)} sales records inserted into the sales collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49be45f-4fbd-45a4-9490-d95aaf84c79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 customer records inserted into MongoDB 'customer_data' collection.\n"
     ]
    }
   ],
   "source": [
    "collection=db['customer_data']\n",
    "# Step 2: Define parameters for synthetic data generation\n",
    "num_customers = 1000  # Number of customer records to generate\n",
    "\n",
    "# Step 3: Generate synthetic customer data\n",
    "data = {\n",
    "    \"Customer ID\": [f\"CUST{str(i).zfill(4)}\" for i in range(1, num_customers + 1)],\n",
    "    \"Average Order Value\": np.round(np.random.uniform(10, 200, num_customers), 2),\n",
    "    \"Purchase Frequency\": np.random.randint(1, 20, num_customers),\n",
    "    \"Days Since Last Purchase\": np.random.randint(1, 365, num_customers),\n",
    "    \"Total Orders\": np.random.randint(1, 100, num_customers),\n",
    "    \"Customer Tenure\": [random.randint(1, 5) for _ in range(num_customers)],  # Tenure in years\n",
    "    \"Engagement Level\": np.random.randint(1, 100, num_customers)  # Assume engagement scores from 1-100\n",
    "}\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 4: Add derived fields\n",
    "# Calculate 'Retention Rate' as a ratio of (Purchase Frequency / Customer Tenure)\n",
    "df['Retention Rate'] = df['Purchase Frequency'] / (df['Customer Tenure'] * 12)\n",
    "\n",
    "# Convert to dictionary format for MongoDB upload\n",
    "records = df.to_dict(orient='records')\n",
    "\n",
    "# Step 5: Insert records into MongoDB\n",
    "collection.insert_many(records)\n",
    "print(f\"{len(records)} customer records inserted into MongoDB 'customer_data' collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48155e-b99a-4e5d-90e9-5ac512f8cf30",
   "metadata": {},
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e2b97c-d1f2-45af-8e93-59d3eeb3c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82cc76d-d9f1-4da3-9cf6-bebfeaeb4546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "collection=db['customer_data']\n",
    "\n",
    "# Assuming data is retrieved from MongoDB into DataFrame `data`\n",
    "data = pd.DataFrame(list(collection.find({}, {\"Customer ID\": 1, \"Purchase Frequency\": 1, \"Average Order Value\": 1, \n",
    "                                              \"Days Since Last Purchase\": 1, \"Total Orders\": 1, \"Customer Tenure\": 1})))\n",
    "\n",
    "# RFM Scoring\n",
    "data['RFM_Score'] = data[['Purchase Frequency', 'Average Order Value', 'Days Since Last Purchase']].mean(axis=1)\n",
    "\n",
    "# Clustering based on RFM score and other features\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "data['Retention Risk Profile'] = kmeans.fit_predict(data[['RFM_Score', 'Customer Tenure', 'Total Orders']])\n",
    "\n",
    "# Update MongoDB with the 'Retention Risk Profile'\n",
    "for index, row in data.iterrows():\n",
    "    collection.update_one({\"Customer ID\": row[\"Customer ID\"]},\n",
    "                          {\"$set\": {\"Retention Risk Profile\": int(row[\"Retention Risk Profile\"])}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1573c83-461e-4f8d-a39c-03213a2e1a27",
   "metadata": {},
   "source": [
    "# Churn likehood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146a12d7-cb31-4215-ae42-2d70fa68f0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1000 records to MongoDB collection 'churn_analysis'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "collection = db[\"churn_analysis\"]\n",
    "# Step 2: Generate mock data\n",
    "num_records = 1000  # Number of records to generate\n",
    "\n",
    "# Create mock data\n",
    "data = {\n",
    "    \"Customer ID\": [f\"CUST{i+1}\" for i in range(num_records)],\n",
    "    \"Engagement Score\": np.random.randint(0, 100, size=num_records),  # Random scores between 0-100\n",
    "    \"Last Interaction\": [datetime.now() - timedelta(days=random.randint(1, 365)) for _ in range(num_records)],  # Random last interaction dates\n",
    "    \"Tenure\": np.random.randint(1, 25, size=num_records),  # Tenure in months\n",
    "    \"Subscription Plan\": random.choices(['Basic', 'Standard', 'Premium'], k=num_records),  # Random subscription plans\n",
    "    \"Churned\": random.choices([0, 1], weights=[0.7, 0.3], k=num_records)  # 30% churned, 70% active\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Upload data to MongoDB\n",
    "# Convert DataFrame to dictionary and insert into the collection\n",
    "collection.insert_many(df.to_dict(orient='records'))\n",
    "\n",
    "print(f\"Uploaded {num_records} records to MongoDB collection '{collection.name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b49e3a2-3d76-4648-9944-b15813796ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.645\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.78       147\n",
      "           1       0.20      0.11      0.14        53\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.46      0.47      0.46       200\n",
      "weighted avg       0.58      0.65      0.61       200\n",
      "\n",
      "  Customer ID  Churn Likelihood\n",
      "0       CUST1              0.07\n",
      "1       CUST2              0.10\n",
      "2       CUST3              0.19\n",
      "3       CUST4              0.07\n",
      "4       CUST5              0.06\n",
      "Churn Likelihood has been uploaded to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Connect to MongoDB and retrieve data\n",
    "\n",
    "# Step 2: Load data into a DataFrame\n",
    "data = pd.DataFrame(list(collection.find({}, {\n",
    "    \"Customer ID\": 1,\n",
    "    \"Engagement Score\": 1,\n",
    "    \"Last Interaction\": 1,\n",
    "    \"Tenure\": 1,\n",
    "    \"Subscription Plan\": 1,\n",
    "    \"Churned\": 1  # Churn labels (1 for churned, 0 for active)\n",
    "})))\n",
    "\n",
    "# Step 3: Prepare the features and target variable\n",
    "# Convert dates and handle categorical variables\n",
    "data['Last Interaction'] = pd.to_datetime(data['Last Interaction'])\n",
    "data['Recency'] = (pd.Timestamp.now() - data['Last Interaction']).dt.days\n",
    "data['Churned'] = data['Churned'].astype(int)  # Ensure churn labels are integers\n",
    "\n",
    "# Select features and target\n",
    "X = data[[\"Engagement Score\", \"Recency\", \"Tenure\"]]  # Add more features as needed\n",
    "y = data[\"Churned\"]  # Target variable\n",
    "\n",
    "# Step 4: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 8: Calculate Churn Likelihood\n",
    "data['Churn Likelihood'] = model.predict_proba(X)[:, 1]  # Probability of churn\n",
    "print(data[['Customer ID', 'Churn Likelihood']].head())  # Display churn likelihood for each customer\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    collection.update_one(\n",
    "        {\"Customer ID\": row[\"Customer ID\"]},\n",
    "        {\"$set\": {\"Churn Likelihood\": float(row[\"Churn Likelihood\"])}}  # Update the Churn Likelihood\n",
    "    )\n",
    "\n",
    "print(\"Churn Likelihood has been uploaded to MongoDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a89a73-9cec-4999-9295-baf81711d2ba",
   "metadata": {},
   "source": [
    "# Customer lifetime value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "258e2733-88e5-4809-8ac5-d9f46fee6fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 transaction records have been generated and inserted into the MongoDB collection.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "collection =db['transactions']\n",
    "# Step 2: Generate sample transaction data\n",
    "num_records = 100  # Define how many records you want to create\n",
    "customers = [f\"CUST{i+1}\" for i in range(10)]  # Sample Customer IDs\n",
    "\n",
    "# Clear the existing data in the collection (optional)\n",
    "collection.delete_many({})\n",
    "\n",
    "for _ in range(num_records):\n",
    "    customer_id = random.choice(customers)\n",
    "    transaction_amount = round(random.uniform(10.0, 1000.0), 2)  # Random amount between 10 and 1000\n",
    "    transaction_date = datetime.now() - timedelta(days=random.randint(0, 365))  # Random date within the last year\n",
    "    \n",
    "    transaction_record = {\n",
    "        \"Customer ID\": customer_id,\n",
    "        \"Transaction Amount\": transaction_amount,\n",
    "        \"Transaction Date\": transaction_date\n",
    "    }\n",
    "    \n",
    "    # Insert the record into the collection\n",
    "    collection.insert_one(transaction_record)\n",
    "\n",
    "print(f\"{num_records} transaction records have been generated and inserted into the MongoDB collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b086f222-cd6d-42a1-b2a9-65d1148933a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Lifetime Values (CLV) have been updated in the transactions collection based on Customer ID.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to MongoDB\n",
    "# client = MongoClient(\"your_mongodb_connection_string\")  # Replace with your MongoDB connection string\n",
    "# db = client[\"your_database_name\"]  # Replace with your database name\n",
    "transactions_collection = db[\"transactions\"]  # Your existing transactions collection\n",
    "\n",
    "# Step 2: Load transaction data into a DataFrame\n",
    "data = pd.DataFrame(list(transactions_collection.find({}, {\n",
    "    \"Customer ID\": 1,\n",
    "    \"Transaction Amount\": 1,\n",
    "    \"Transaction Date\": 1\n",
    "})))\n",
    "\n",
    "# Step 3: Calculate Average Purchase Value (APV)\n",
    "total_revenue = data['Transaction Amount'].sum()\n",
    "total_purchases = data.shape[0]\n",
    "APV = total_revenue / total_purchases if total_purchases > 0 else 0  # Handle division by zero\n",
    "\n",
    "# Step 4: Calculate Average Purchase Frequency (APF)\n",
    "total_unique_customers = data['Customer ID'].nunique()\n",
    "APF = total_purchases / total_unique_customers if total_unique_customers > 0 else 0  # Handle division by zero\n",
    "\n",
    "# Step 5: Calculate Customer Lifespan (CL)\n",
    "data['Transaction Date'] = pd.to_datetime(data['Transaction Date'])\n",
    "customer_lifespans = data.groupby('Customer ID')['Transaction Date'].agg(['min', 'max'])\n",
    "customer_lifespans['CL'] = (customer_lifespans['max'] - customer_lifespans['min']).dt.days / 365  # in years\n",
    "\n",
    "# Step 6: Merge CL back into the original data\n",
    "data = data.merge(customer_lifespans[['CL']], on='Customer ID', how='left')\n",
    "\n",
    "# Step 7: Calculate Customer Lifetime Value (CLV) for each customer\n",
    "data['CLV'] = APV * APF * data['CL']  # Use individual customer's CL for calculation\n",
    "data['CLV'] = MinMaxScaler().fit_transform(data[['CLV']])\n",
    "\n",
    "# Step 8: Update the CLV into the transactions collection based on Customer ID\n",
    "for customer_id, group in data.groupby('Customer ID'):\n",
    "    clv_value = group['CLV'].iloc[0]  # Get CLV for the customer\n",
    "    transactions_collection.update_many(\n",
    "        {\"Customer ID\": customer_id},  # Match by Customer ID\n",
    "        {\"$set\": {\"CLV\": clv_value}}   # Update CLV field\n",
    "    )\n",
    "\n",
    "print(\"Customer Lifetime Values (CLV) have been updated in the transactions collection based on Customer ID.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0882b32-e9c7-4080-ad6d-1dcdac45d262",
   "metadata": {},
   "source": [
    "# Lead Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d76aecda-0dd0-41bd-9664-9e1a76eb837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 new leads have been generated and inserted into the Leads collection.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Connect to MongoDB\n",
    "collection = db['Leads']  # Your leads collection\n",
    "\n",
    "# Step 2: Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Function to create a single lead\n",
    "def create_lead():\n",
    "    return {\n",
    "        \"lead_id\": str(fake.uuid4()),  # Unique lead ID\n",
    "        \"name\": fake.name(),            # Random name\n",
    "        \"email\": fake.email(),          # Random email\n",
    "        \"phone\": fake.phone_number(),    # Random phone number\n",
    "        \"created_at\": fake.date_time_this_year(),  # Created at date\n",
    "        \"status\": random.choice([\"Contacted\", \"Not Contacted\", \"Interested\", \"Not Interested\"]),  # Random status\n",
    "        \"isActive\": random.choice([True, False])  # Randomly set as active or not\n",
    "    }\n",
    "\n",
    "# Step 3: Generate 200 leads\n",
    "new_leads = [create_lead() for _ in range(200)]\n",
    "\n",
    "# Step 4: Insert leads into the MongoDB collection\n",
    "collection.insert_many(new_leads)\n",
    "\n",
    "print(\"200 new leads have been generated and inserted into the Leads collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6af98c4c-3052-4bc9-b386-4a544d86cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total Leads  Active Leads  \\\n",
      "0          200            96   \n",
      "\n",
      "                                        Lead Scoring  \n",
      "0  {'Contacted': 48, 'Interested': 54, 'Not Conta...  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Calculate Total Leads\n",
    "total_leads = collection.count_documents({})\n",
    "\n",
    "# Step 3: Calculate Active Leads\n",
    "active_leads = collection.count_documents({\"isActive\": True})\n",
    "\n",
    "# Step 4: Calculate Lead Scoring (Example: assigning score based on status)\n",
    "lead_scoring = collection.aggregate([\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$status\",  # Group by status\n",
    "            \"count\": {\"$sum\": 1},  # Count leads per status\n",
    "            \"score\": {\"$sum\": 1}  # Assign a score (this can be modified as needed)\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "# Convert lead scoring result to a dictionary\n",
    "lead_scoring_dict = {item['_id']: item['count'] for item in lead_scoring}\n",
    "\n",
    "# Step 5: Create a DataFrame for results\n",
    "result_data = {\n",
    "    \"Total Leads\": total_leads,\n",
    "    \"Active Leads\": active_leads,\n",
    "    \"Lead Scoring\": lead_scoring_dict\n",
    "}\n",
    "\n",
    "# Convert result to DataFrame\n",
    "df_results = pd.DataFrame([result_data])\n",
    "\n",
    "# Print the results\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94464bf4-5177-4ef5-a628-89e4a749c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 fake leads inserted into the database.\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import pymongo\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# MongoDB connection\n",
    "leads_collection = db[\"Leads\"]\n",
    "\n",
    "def generate_fake_lead():\n",
    "    return {\n",
    "        \"lead_id\": str(fake.uuid4()),\n",
    "        \"name\": fake.name(),\n",
    "        \"email\": fake.email(),\n",
    "        \"phone\": fake.phone_number(),\n",
    "        \"created_at\": fake.date_time_this_year().isoformat(),\n",
    "        \"status\": random.choice([\"Interested\", \"Contacted\", \"Not Interested\"]),\n",
    "        \"isActive\": random.choice([True, False]),\n",
    "        \"email_open_count\": random.randint(0, 20),  # Fake engagement data\n",
    "        \"website_visit_count\": random.randint(0, 15),  # Fake engagement data\n",
    "        \"company_size\": random.randint(1, 200),  # Random company size between 1 and 200\n",
    "        \"last_contact_date\": fake.date_time_this_year().isoformat() if random.choice([True, False]) else None  # Random last contact date\n",
    "    }\n",
    "\n",
    "# Generate and insert fake leads\n",
    "num_fake_leads = 100  # Number of fake leads to generate\n",
    "for _ in range(num_fake_leads):\n",
    "    lead = generate_fake_lead()\n",
    "    leads_collection.insert_one(lead)\n",
    "\n",
    "print(f\"{num_fake_leads} fake leads inserted into the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8eb809-c6a6-4321-82b3-fc30a88275fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_leads(leads):\n",
    "    scored_leads = []\n",
    "    \n",
    "    for lead in leads:\n",
    "        score = 0\n",
    "        \n",
    "        # Status scoring\n",
    "        if lead.get('status') == 'Interested':\n",
    "            score += 10\n",
    "        elif lead.get('status') == 'Contacted':\n",
    "            score += 5\n",
    "            \n",
    "        # Engagement metrics\n",
    "        score += lead.get('email_open_count', 0) * 2  # +2 for each email opened\n",
    "        score += lead.get('website_visit_count', 0) * 3  # +3 for each website visit\n",
    "        \n",
    "        # Company size scoring\n",
    "        company_size = lead.get('company_size', 0)\n",
    "        if company_size > 100:\n",
    "            score += 10\n",
    "        elif company_size > 50:\n",
    "            score += 5\n",
    "        \n",
    "        # Recency of last contact\n",
    "        last_contact_date = lead.get('last_contact_date')\n",
    "        if last_contact_date:\n",
    "            last_contact_date = datetime.fromisoformat(last_contact_date)  # Adjust format as needed\n",
    "            days_since_contact = (datetime.now() - last_contact_date).days\n",
    "            \n",
    "            if days_since_contact <= 30:\n",
    "                score += 5  # Contacted in the last 30 days\n",
    "            elif days_since_contact <= 60:\n",
    "                score += 3  # Contacted in the last 60 days\n",
    "        \n",
    "        # Add lead score to the lead dictionary\n",
    "        scored_leads.append({**lead, 'score': score})\n",
    "    \n",
    "    return scored_leads\n",
    "\n",
    "# Fetch leads from MongoDB and score them\n",
    "leads_data = list(leads_collection.find())\n",
    "scored_leads = score_leads(leads_data)\n",
    "\n",
    "# Sort leads by score in descending order\n",
    "scored_leads.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# Print scored leads\n",
    "for lead in scored_leads:\n",
    "    leads_collection.update_one(\n",
    "        {\"lead_id\": lead[\"lead_id\"]},  # Find the lead by lead_id\n",
    "        {\"$set\": {\"score\": lead[\"score\"]}}  # Update the score field\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7981a6c-b0e4-459a-84d9-3ce809c14c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48., 48.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a193a-bc02-4fd9-9d20-3f39776fd203",
   "metadata": {},
   "source": [
    "## Customer Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2abadb-11a3-45a8-971c-faf7cc077446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 200 records into the CustomerChurnData collection.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Faker and MongoDB client\n",
    "fake = Faker()\n",
    "collection = db[\"CustomerChurnData\"]  # New collection\n",
    "\n",
    "# Generate fake data\n",
    "data = []\n",
    "for _ in range(200):\n",
    "    customer_id = fake.uuid4()\n",
    "    name = fake.name()\n",
    "    email = fake.email()\n",
    "    phone = fake.phone_number()\n",
    "    created_at = fake.date_time_between(start_date='-2y', end_date='now')\n",
    "    status = random.choice([\"Active\", \"Inactive\", \"Churned\"])\n",
    "    is_active = True if status == \"Active\" else False\n",
    "    subscription_type = random.choice([\"Monthly\", \"Annual\"])\n",
    "    last_login = created_at + timedelta(days=random.randint(10, 730))\n",
    "    total_usage = random.randint(1, 500)  # Example usage metric\n",
    "\n",
    "    # Add to data list\n",
    "    data.append({\n",
    "        \"customer_id\": customer_id,\n",
    "        \"name\": name,\n",
    "        \"email\": email,\n",
    "        \"phone\": phone,\n",
    "        \"created_at\": created_at,\n",
    "        \"status\": status,\n",
    "        \"is_active\": is_active,\n",
    "        \"subscription_type\": subscription_type,\n",
    "        \"last_login\": last_login,\n",
    "        \"total_usage\": total_usage\n",
    "    })\n",
    "\n",
    "# Insert data into MongoDB\n",
    "insert_result = collection.insert_many(data)\n",
    "print(f\"Inserted {len(insert_result.inserted_ids)} records into the CustomerChurnData collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689872d7-26a9-4f79-9b47-13e727444118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.675\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78        82\n",
      "           1       0.48      0.34      0.40        38\n",
      "\n",
      "    accuracy                           0.68       120\n",
      "   macro avg       0.61      0.59      0.59       120\n",
      "weighted avg       0.65      0.68      0.66       120\n",
      "\n",
      "Predictions saved to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# MongoDB connection and data loading\n",
    "collection = db[\"CustomerChurnData\"]\n",
    "data = pd.DataFrame(list(collection.find()))\n",
    "data.drop(\"_id\", axis=1, inplace=True)\n",
    "\n",
    "# Data preprocessing\n",
    "data[\"created_at\"] = pd.to_datetime(data[\"created_at\"])\n",
    "data[\"last_login\"] = pd.to_datetime(data[\"last_login\"])\n",
    "data[\"days_since_last_login\"] = (pd.to_datetime(\"now\") - data[\"last_login\"]).dt.days\n",
    "data[\"churn\"] = data[\"status\"].apply(lambda x: 1 if x == \"Churned\" else 0)\n",
    "data = data.drop([\"customer_id\", \"name\", \"email\", \"phone\", \"status\", \"last_login\", \"created_at\"], axis=1)\n",
    "data = pd.get_dummies(data, columns=[\"subscription_type\"], drop_first=True)\n",
    "\n",
    "# Train-test split\n",
    "X = data.drop(\"churn\", axis=1)\n",
    "y = data[\"churn\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save predictions to MongoDB\n",
    "X_test[\"predicted_churn\"] = y_pred\n",
    "X_test[\"actual_churn\"] = y_test.values\n",
    "predictions_data = X_test.to_dict(\"records\")\n",
    "predictions_collection = db[\"ChurnPredictions\"]\n",
    "# predictions_collection.insert_many(predictions_data)\n",
    "import joblib\n",
    "joblib.dump(model, 'model_filename.joblib')\n",
    "\n",
    "print(\"Predictions saved to MongoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "469aa9e7-b109-4626-8df1-2ca599758c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from collections\n",
    "feedback_collection = db[\"feedback\"]\n",
    "sales_collection = db[\"sales\"]\n",
    "leads_collection = db[\"Leads\"]\n",
    "\n",
    "# Load data into DataFrames\n",
    "feedback_data = pd.DataFrame(list(feedback_collection.find()))\n",
    "sales_data = pd.DataFrame(list(sales_collection.find()))\n",
    "leads_data = pd.DataFrame(list(leads_collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dad1f179-5edc-4282-8245-39d0c92e3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Clean feedback reviews\n",
    "feedback_data['cleaned_review'] = feedback_data['Review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "683d3e00-5ac7-4444-8661-bbf4a045804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize the cleaned reviews\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)  # Limit to top 500 features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(feedback_data['cleaned_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a7f004e-47d0-443d-a395-1c4c6fc7b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'product_id' exists in both feedback and sales data\n",
    "merged_data = feedback_data.merge(sales_data, on='_id', how='left')\n",
    "# merged_data = merged_data.merge(leads_data, on='_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d26645b-4388-47b1-a71e-a85edcd4f205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 Keywords: okay, average, product, bad, quality\n",
      "Average sentiment for topic 0: 0.44760101010101017\n",
      "Topic 1 Keywords: alright, meets, service, outstanding, satisfactory\n",
      "Average sentiment for topic 1: 0.13446712018140589\n",
      "Topic 2 Keywords: low, satisfied, happy, extremely, purchase\n",
      "Average sentiment for topic 2: 0.4625\n",
      "Topic 3 Keywords: shipping, recommend, expected, highly, disappointed\n",
      "Average sentiment for topic 3: -0.04433760683760687\n",
      "Recommendation: Investigate low sentiment for topic '['shipping', 'recommend', 'expected', 'highly', 'disappointed']' and cross-check with low sales.\n",
      "Topic 4 Keywords: price, product, expect, terrible, money\n",
      "Average sentiment for topic 4: -0.4647727272727273\n",
      "Recommendation: Investigate low sentiment for topic '['price', 'product', 'expect', 'terrible', 'money']' and cross-check with low sales.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess feedback data\n",
    "feedback_data['cleaned_feedback'] = feedback_data['Review'].str.lower()  # Lowercase\n",
    "\n",
    "# Step 3: Extract topics from feedback data using TF-IDF and LDA\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "feedback_tfidf = vectorizer.fit_transform(feedback_data['cleaned_feedback'])\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(feedback_tfidf)\n",
    "\n",
    "# Get main keywords for each topic\n",
    "keywords = []\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    keywords.append([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]])\n",
    "\n",
    "# Step 4: Add topic and sentiment labels to feedback data\n",
    "def get_topic(text, vectorizer=vectorizer, model=lda):\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    topic_idx = model.transform(text_tfidf).argmax()\n",
    "    return topic_idx\n",
    "\n",
    "feedback_data['topic'] = feedback_data['cleaned_feedback'].apply(get_topic)\n",
    "feedback_data['sentiment'] = feedback_data['Review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Step 5: Group by topics and analyze patterns with sales data\n",
    "for topic_idx, words in enumerate(keywords):\n",
    "    print(f\"Topic {topic_idx} Keywords: {', '.join(words)}\")\n",
    "    topic_data = feedback_data[feedback_data['topic'] == topic_idx]\n",
    "    \n",
    "    # Example aggregation: Average sentiment for the topic\n",
    "    avg_sentiment = topic_data['sentiment'].mean()\n",
    "    print(f\"Average sentiment for topic {topic_idx}: {avg_sentiment}\")\n",
    "    \n",
    "    # Step 6: Analyze patterns and compare with sales data (example for sales insights)\n",
    "    # Example: Identify sales regions with low performance if sentiment is negative\n",
    "    if avg_sentiment < 0:\n",
    "        print(f\"Recommendation: Investigate low sentiment for topic '{words}' and cross-check with low sales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cba4e1f-d83f-453b-ac82-693fde3a7b88",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_data\u001b[38;5;241m.\u001b[39miloc[top_products_indices]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Example: Get recommendations for a specific product_id\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m recommended_products \u001b[38;5;241m=\u001b[39m get_recommendations(product_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m, in \u001b[0;36mget_recommendations\u001b[1;34m(product_id, cosine_sim)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_recommendations\u001b[39m(product_id, cosine_sim\u001b[38;5;241m=\u001b[39mcosine_sim):\n\u001b[1;32m----> 8\u001b[0m     idx \u001b[38;5;241m=\u001b[39m merged_data[merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m product_id]\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m     sim_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(cosine_sim[idx]))\n\u001b[0;32m     10\u001b[0m     sim_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(sim_scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5389\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5387\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5388\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[1;32m-> 5389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m getitem(key)\n\u001b[0;32m   5391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   5392\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5393\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[0;32m   5394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity based on TF-IDF matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Function to get recommendations based on review similarity\n",
    "def get_recommendations(product_id, cosine_sim=cosine_sim):\n",
    "    idx = merged_data[merged_data['product_id'] == product_id].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get top 10 most similar products\n",
    "    top_products_indices = [i[0] for i in sim_scores[1:11]]\n",
    "    return merged_data.iloc[top_products_indices]\n",
    "\n",
    "# Example: Get recommendations for a specific product_id\n",
    "recommended_products = get_recommendations(product_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d139c03-22a8-43b0-9002-6eba71e006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05daf3a1-d35c-4a46-a110-79504ea5b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Connect to MongoDB\n",
    "feedback_collection = db[\"feedback\"]\n",
    "sales_collection = db[\"sales\"]\n",
    "\n",
    "# Load data into DataFrame\n",
    "feedback_df = pd.DataFrame(list(feedback_collection.find()))\n",
    "sales_df = pd.DataFrame(list(sales_collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8d13271-b194-45d3-911f-14579a7f8d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top products for segment 1: ['P039', 'P007', 'P016', 'P046', 'P049']\n",
      "Top products for segment 2: ['P045', 'P029', 'P010', 'P040', 'P035']\n",
      "Top products for segment 0: ['P021', 'P046', 'P044', 'P035', 'P017']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jainn\\AppData\\Local\\Temp\\ipykernel_17956\\431850522.py:34: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  product_matrix = product_matrix.applymap(lambda x: 1 if x > 0 else 0)  # Convert to binary\n",
      "C:\\Users\\jainn\\anaconda3\\envs\\notebook\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Step 1: Connect to MongoDB and load sales data\n",
    "# client = MongoClient(\"your_mongodb_connection_string\")\n",
    "# db = client[\"your_database_name\"]\n",
    "sales_collection = db[\"sales\"]\n",
    "sales_df = pd.DataFrame(list(sales_collection.find()))\n",
    "\n",
    "# Step 2: Preprocess data for customer segmentation and affinity analysis\n",
    "# sales_df['sale_date'] = pd.to_datetime(sales_df['sale_date'])\n",
    "# sales_df['year_month'] = sales_df['sale_date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate revenue per customer for segmentation\n",
    "customer_sales = sales_df.groupby('customer_id').agg({\n",
    "    'quantity': 'sum',\n",
    "    'revenue': 'sum',\n",
    "    'product_id': 'nunique'\n",
    "}).reset_index()\n",
    "customer_sales.columns = ['customer_id', 'total_quantity', 'total_revenue', 'unique_products']\n",
    "\n",
    "# Step 3: Customer Segmentation using K-Means\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(customer_sales[['total_quantity', 'total_revenue', 'unique_products']])\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # Adjust clusters as needed\n",
    "customer_sales['segment'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Step 4: Product Affinity Analysis with Apriori\n",
    "# Create product-transaction matrix\n",
    "product_matrix = sales_df.pivot_table(index='sale_id', columns='product_id', values='quantity', aggfunc='sum').fillna(0)\n",
    "product_matrix = product_matrix.applymap(lambda x: 1 if x > 0 else 0)  # Convert to binary\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "frequent_itemsets = apriori(product_matrix, min_support=0.01, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "# Step 5: Generating Product Recommendations\n",
    "# Example recommendation based on segment\n",
    "recommendations = {}\n",
    "for segment in customer_sales['segment'].unique():\n",
    "    segment_products = sales_df[sales_df['customer_id'].isin(\n",
    "        customer_sales[customer_sales['segment'] == segment]['customer_id'])]['product_id']\n",
    "    popular_products = segment_products.value_counts().head(5).index.tolist()\n",
    "    recommendations[segment] = popular_products\n",
    "\n",
    "# Output recommendations\n",
    "for segment, products in recommendations.items():\n",
    "    print(f\"Top products for segment {segment}: {products}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7986eb1-e33f-4ff4-bd19-422d99aa92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jainn\\anaconda3\\envs\\notebook\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.029703</td>\n",
       "      <td>(P010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P020)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P025)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.029703</td>\n",
       "      <td>(P029)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P033)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P037)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.049505</td>\n",
       "      <td>(P039)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P041)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P043)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P044)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.029703</td>\n",
       "      <td>(P045)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>(P046)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.029703</td>\n",
       "      <td>(P047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P048)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.019802</td>\n",
       "      <td>(P049)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.029703</td>\n",
       "      <td>(P050)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     support itemsets\n",
       "0   0.019802   (P001)\n",
       "1   0.039604   (P002)\n",
       "2   0.019802   (P003)\n",
       "3   0.019802   (P007)\n",
       "4   0.039604   (P009)\n",
       "5   0.029703   (P010)\n",
       "6   0.019802   (P016)\n",
       "7   0.019802   (P017)\n",
       "8   0.019802   (P018)\n",
       "9   0.039604   (P020)\n",
       "10  0.039604   (P021)\n",
       "11  0.019802   (P022)\n",
       "12  0.019802   (P023)\n",
       "13  0.019802   (P025)\n",
       "14  0.019802   (P026)\n",
       "15  0.029703   (P029)\n",
       "16  0.019802   (P030)\n",
       "17  0.019802   (P033)\n",
       "18  0.019802   (P034)\n",
       "19  0.039604   (P035)\n",
       "20  0.039604   (P037)\n",
       "21  0.049505   (P039)\n",
       "22  0.019802   (P040)\n",
       "23  0.019802   (P041)\n",
       "24  0.019802   (P043)\n",
       "25  0.039604   (P044)\n",
       "26  0.029703   (P045)\n",
       "27  0.039604   (P046)\n",
       "28  0.029703   (P047)\n",
       "29  0.019802   (P048)\n",
       "30  0.019802   (P049)\n",
       "31  0.029703   (P050)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets = apriori(product_matrix, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9f35d946-6845-4df1-83de-f20c291883c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jainn\\anaconda3\\envs\\notebook\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\jainn\\anaconda3\\envs\\notebook\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\jainn\\anaconda3\\envs\\notebook\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\jainn\\anaconda3\\envs\\notebook\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'product_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'product_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clusters):\n\u001b[0;32m     48\u001b[0m     cluster_reviews \u001b[38;5;241m=\u001b[39m feedback_data[feedback_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m cluster_id]\n\u001b[1;32m---> 49\u001b[0m     cluster_products \u001b[38;5;241m=\u001b[39m cluster_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Aggregate sentiment for the products in the cluster\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     sentiment_summary \u001b[38;5;241m=\u001b[39m cluster_reviews\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'product_id'"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load spaCy model for advanced NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# MongoDB connection\n",
    "\n",
    "feedback_collection = db[\"feedback\"]\n",
    "sales_collection = db[\"sales\"]\n",
    "\n",
    "# Fetch feedback and sales data\n",
    "feedback_data = pd.DataFrame(list(feedback_collection.find({}, {\"Review\": 1})))\n",
    "sales_data = pd.DataFrame(list(sales_collection.find({}, {\"product_id\": 1, \"revenue\": 1})))\n",
    "\n",
    "# Step 1: Preprocessing with spaCy\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "feedback_data['cleaned_text'] = feedback_data['Review'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Feature extraction\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(feedback_data['cleaned_text'])\n",
    "\n",
    "# Step 3: Clustering reviews\n",
    "num_clusters = 5  # You can adjust the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "feedback_data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Step 4: Calculate sentiment for each review\n",
    "feedback_data['sentiment'] = feedback_data['cleaned_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Step 5: Analyze sales data\n",
    "sales_summary = sales_data.groupby('product_id').agg({'revenue': 'sum', '_id': 'count'}).reset_index()\n",
    "sales_summary.columns = ['product_id', 'total_revenue', 'sales_count']\n",
    "\n",
    "# Step 6: Recommendations based on clusters and sales\n",
    "recommendations = {}\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_reviews = feedback_data[feedback_data['cluster'] == cluster_id]\n",
    "    cluster_products = cluster_reviews['product_id'].unique()\n",
    "    \n",
    "    # Aggregate sentiment for the products in the cluster\n",
    "    sentiment_summary = cluster_reviews.groupby('product_id')['sentiment'].mean().reset_index()\n",
    "    \n",
    "    # Merge sales data with sentiment\n",
    "    cluster_data = pd.merge(sentiment_summary, sales_summary, on='product_id', how='inner')\n",
    "    \n",
    "    # Sort by revenue and sentiment\n",
    "    recommendations[cluster_id] = cluster_data.sort_values(by=['total_revenue', 'sentiment'], ascending=[False, False])\n",
    "\n",
    "# Print recommendations in a more realistic format\n",
    "for cluster_id, recs in recommendations.items():\n",
    "    print(f\"Recommendations for Cluster {cluster_id}:\")\n",
    "    for _, row in recs.iterrows():\n",
    "        sentiment_description = \"positively received\" if row['sentiment'] > 0 else \"negatively received\"\n",
    "        print(f\"  - Product ID: {row['product_id']}, Total Revenue: {row['total_revenue']}, Sentiment: {sentiment_description}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "652a4251-a79c-48c8-9e4e-3d2c38ea6c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>671d1f4bb9237df0b8063b9a</td>\n",
       "      <td>P026</td>\n",
       "      <td>1242.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>671d1f4bb9237df0b8063b99</td>\n",
       "      <td>P021</td>\n",
       "      <td>2025.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>671d1f4bb9237df0b8063b9b</td>\n",
       "      <td>P016</td>\n",
       "      <td>347.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>671d1f4bb9237df0b8063ba0</td>\n",
       "      <td>P050</td>\n",
       "      <td>1383.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671d1f4bb9237df0b8063ba2</td>\n",
       "      <td>P045</td>\n",
       "      <td>2917.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>671d1f4bb9237df0b8063be3</td>\n",
       "      <td>P025</td>\n",
       "      <td>1214.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>671d1f4bb9237df0b8063be7</td>\n",
       "      <td>P039</td>\n",
       "      <td>297.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>671d1f4bb9237df0b8063bf4</td>\n",
       "      <td>P048</td>\n",
       "      <td>408.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>671d1f4bb9237df0b8063bf7</td>\n",
       "      <td>P010</td>\n",
       "      <td>1362.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>671d5c2e9ad959a54db96d1b</td>\n",
       "      <td>P029</td>\n",
       "      <td>97.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          _id product_id  revenue\n",
       "0    671d1f4bb9237df0b8063b9a       P026  1242.54\n",
       "1    671d1f4bb9237df0b8063b99       P021  2025.65\n",
       "2    671d1f4bb9237df0b8063b9b       P016   347.60\n",
       "3    671d1f4bb9237df0b8063ba0       P050  1383.03\n",
       "4    671d1f4bb9237df0b8063ba2       P045  2917.80\n",
       "..                        ...        ...      ...\n",
       "96   671d1f4bb9237df0b8063be3       P025  1214.72\n",
       "97   671d1f4bb9237df0b8063be7       P039   297.84\n",
       "98   671d1f4bb9237df0b8063bf4       P048   408.30\n",
       "99   671d1f4bb9237df0b8063bf7       P010  1362.30\n",
       "100  671d5c2e9ad959a54db96d1b       P029    97.60\n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6adee60-879b-43c8-8af7-004dffc966d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 187.9 kB/s eta 0:01:08\n",
      "     --------------------------------------- 0.1/12.8 MB 297.7 kB/s eta 0:00:43\n",
      "     --------------------------------------- 0.1/12.8 MB 655.8 kB/s eta 0:00:20\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.6/12.8 MB 10.0 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.9/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 13.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 14.3 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 10.9/12.8 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.2/12.8 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 20.5 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1bfad736-3ca2-43cf-93d0-2aa53319e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 new entries into the collection!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize MongoDB client\n",
    "# client = MongoClient(\"mongodb://localhost:27017/\")  # Replace with your MongoDB URI if necessary\n",
    "# db = client['your_database_name']  # Replace with your database name\n",
    "collection = db['feedback']  # Replace with your collection name\n",
    "\n",
    "# Initialize Faker and define a review template\n",
    "fake = Faker()\n",
    "\n",
    "# Generate 100 new entries\n",
    "data = []\n",
    "for _ in range(100):\n",
    "    entry = {\n",
    "        \"Review\": fake.sentence(nb_words=5),  # Generate a random short review sentence\n",
    "        \"Rating\": random.choice([4, 5])  # Randomly choose between high ratings\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "# Insert the data into MongoDB\n",
    "collection.insert_many(data)\n",
    "\n",
    "print(\"Inserted 100 new entries into the collection!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b3f7b874-f84e-4391-8f69-888660a031c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>671d1f4bb9237df0b8063b9a</td>\n",
       "      <td>P026</td>\n",
       "      <td>1242.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>671d1f4bb9237df0b8063b99</td>\n",
       "      <td>P021</td>\n",
       "      <td>2025.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>671d1f4bb9237df0b8063b9b</td>\n",
       "      <td>P016</td>\n",
       "      <td>347.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>671d1f4bb9237df0b8063ba0</td>\n",
       "      <td>P050</td>\n",
       "      <td>1383.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671d1f4bb9237df0b8063ba2</td>\n",
       "      <td>P045</td>\n",
       "      <td>2917.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>671d1f4bb9237df0b8063be3</td>\n",
       "      <td>P025</td>\n",
       "      <td>1214.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>671d1f4bb9237df0b8063be7</td>\n",
       "      <td>P039</td>\n",
       "      <td>297.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>671d1f4bb9237df0b8063bf4</td>\n",
       "      <td>P048</td>\n",
       "      <td>408.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>671d1f4bb9237df0b8063bf7</td>\n",
       "      <td>P010</td>\n",
       "      <td>1362.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>671d5c2e9ad959a54db96d1b</td>\n",
       "      <td>P029</td>\n",
       "      <td>97.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          _id product_id  revenue\n",
       "0    671d1f4bb9237df0b8063b9a       P026  1242.54\n",
       "1    671d1f4bb9237df0b8063b99       P021  2025.65\n",
       "2    671d1f4bb9237df0b8063b9b       P016   347.60\n",
       "3    671d1f4bb9237df0b8063ba0       P050  1383.03\n",
       "4    671d1f4bb9237df0b8063ba2       P045  2917.80\n",
       "..                        ...        ...      ...\n",
       "96   671d1f4bb9237df0b8063be3       P025  1214.72\n",
       "97   671d1f4bb9237df0b8063be7       P039   297.84\n",
       "98   671d1f4bb9237df0b8063bf4       P048   408.30\n",
       "99   671d1f4bb9237df0b8063bf7       P010  1362.30\n",
       "100  671d5c2e9ad959a54db96d1b       P029    97.60\n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
